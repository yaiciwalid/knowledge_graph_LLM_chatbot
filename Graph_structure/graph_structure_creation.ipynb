{"cells":[{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":8694,"status":"ok","timestamp":1709544521292,"user":{"displayName":"Maziz Yassine","userId":"07023992471159053402"},"user_tz":-60},"id":"-cj2hu3hYBIx","outputId":"0c9f72f5-ccd9-4801-e0c9-b82dfec9f038"},"outputs":[{"name":"stdout","output_type":"stream","text":["Collecting wikipedia-api\n","  Downloading Wikipedia_API-0.6.0-py3-none-any.whl (14 kB)\n","Requirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from wikipedia-api) (2.31.0)\n","Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests->wikipedia-api) (3.3.2)\n","Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests->wikipedia-api) (3.6)\n","Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests->wikipedia-api) (2.0.7)\n","Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests->wikipedia-api) (2024.2.2)\n","Installing collected packages: wikipedia-api\n","Successfully installed wikipedia-api-0.6.0\n"]}],"source":["!pip install wikipedia-api\n"]},{"cell_type":"markdown","metadata":{"id":"fhS57g5bCvds"},"source":["## Document_download"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"ibHZA3VZy43i"},"outputs":[],"source":["film_list = [\"Dune: Part Two\",\"Spider-Man: Far From Home\",\"Uncharted (film)\",\"The Great Gatsby (2013 film)\",\"Don't Look Up\",\"Inception\",\"Harry Potter and the Goblet of Fire (film)\",\"Harry Potter and the Philosopher's Stone (film)\",\"Oppenheimer (film)\",\"Interstellar (film)\"]"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"g7vFgNKyYE5w"},"outputs":[],"source":["import wikipediaapi\n","\n","def get_wiki_content(page_title):\n","  user_agent = \"My Wikipedia Scraper (contact@example.com)\"\n","\n","  wiki_wiki = wikipediaapi.Wikipedia(user_agent=user_agent)\n","\n","  page = wiki_wiki.page(page_title)\n","\n","  if page.exists():\n","      return page.text\n","  else:\n","      return None"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"ztAkkhyB4OFr"},"outputs":[],"source":["films_content = []\n","for film in film_list:\n","  films_content.append(get_wiki_content(film))"]},{"cell_type":"markdown","metadata":{"id":"tDJ4h1qSc_S_"},"source":["## Relation_Extraction\n"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":33944,"status":"ok","timestamp":1709554763012,"user":{"displayName":"WALID YAICI","userId":"13150495527287772964"},"user_tz":-60},"id":"HDCNbRvQdFk4","outputId":"fb4663be-ce04-4af2-d875-5bdc826f0b28"},"outputs":[{"name":"stdout","output_type":"stream","text":["Collecting openai\n","  Downloading openai-1.13.3-py3-none-any.whl (227 kB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m227.4/227.4 kB\u001b[0m \u001b[31m4.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hRequirement already satisfied: anyio<5,>=3.5.0 in /usr/local/lib/python3.10/dist-packages (from openai) (3.7.1)\n","Requirement already satisfied: distro<2,>=1.7.0 in /usr/lib/python3/dist-packages (from openai) (1.7.0)\n","Collecting httpx<1,>=0.23.0 (from openai)\n","  Downloading httpx-0.27.0-py3-none-any.whl (75 kB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m75.6/75.6 kB\u001b[0m \u001b[31m8.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hRequirement already satisfied: pydantic<3,>=1.9.0 in /usr/local/lib/python3.10/dist-packages (from openai) (2.6.3)\n","Requirement already satisfied: sniffio in /usr/local/lib/python3.10/dist-packages (from openai) (1.3.1)\n","Requirement already satisfied: tqdm>4 in /usr/local/lib/python3.10/dist-packages (from openai) (4.66.2)\n","Requirement already satisfied: typing-extensions<5,>=4.7 in /usr/local/lib/python3.10/dist-packages (from openai) (4.10.0)\n","Requirement already satisfied: idna>=2.8 in /usr/local/lib/python3.10/dist-packages (from anyio<5,>=3.5.0->openai) (3.6)\n","Requirement already satisfied: exceptiongroup in /usr/local/lib/python3.10/dist-packages (from anyio<5,>=3.5.0->openai) (1.2.0)\n","Requirement already satisfied: certifi in /usr/local/lib/python3.10/dist-packages (from httpx<1,>=0.23.0->openai) (2024.2.2)\n","Collecting httpcore==1.* (from httpx<1,>=0.23.0->openai)\n","  Downloading httpcore-1.0.4-py3-none-any.whl (77 kB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m77.8/77.8 kB\u001b[0m \u001b[31m6.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hCollecting h11<0.15,>=0.13 (from httpcore==1.*->httpx<1,>=0.23.0->openai)\n","  Downloading h11-0.14.0-py3-none-any.whl (58 kB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m58.3/58.3 kB\u001b[0m \u001b[31m6.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hRequirement already satisfied: annotated-types>=0.4.0 in /usr/local/lib/python3.10/dist-packages (from pydantic<3,>=1.9.0->openai) (0.6.0)\n","Requirement already satisfied: pydantic-core==2.16.3 in /usr/local/lib/python3.10/dist-packages (from pydantic<3,>=1.9.0->openai) (2.16.3)\n","Installing collected packages: h11, httpcore, httpx, openai\n","Successfully installed h11-0.14.0 httpcore-1.0.4 httpx-0.27.0 openai-1.13.3\n","Collecting Neo4j\n","  Downloading neo4j-5.18.0.tar.gz (198 kB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m198.0/198.0 kB\u001b[0m \u001b[31m3.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25h  Installing build dependencies ... \u001b[?25l\u001b[?25hdone\n","  Getting requirements to build wheel ... \u001b[?25l\u001b[?25hdone\n","  Installing backend dependencies ... \u001b[?25l\u001b[?25hdone\n","  Preparing metadata (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n","Requirement already satisfied: pytz in /usr/local/lib/python3.10/dist-packages (from Neo4j) (2023.4)\n","Building wheels for collected packages: Neo4j\n","  Building wheel for Neo4j (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n","  Created wheel for Neo4j: filename=neo4j-5.18.0-py3-none-any.whl size=273862 sha256=31f72669d9146c31033384f70fd74062bb891c419a6277e866accb29c627e4f7\n","  Stored in directory: /root/.cache/pip/wheels/e7/e1/a0/dd7c19192f5383ff57d02a6c126cbfe4b7b2ae82f70c6994ce\n","Successfully built Neo4j\n","Installing collected packages: Neo4j\n","Successfully installed Neo4j-5.18.0\n"]}],"source":["!pip install openai\n","!pip install Neo4j"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"vWZIsOaHbKfU"},"outputs":[],"source":["import openai\n","import os\n","import re\n","\n","os.environ['OPENAI_API_KEY'] = \"\"\n","\n","from openai import OpenAI\n","client = OpenAI()\n","\n","def extract_entity_relation(page_text):\n","  relations = \"Ontology of triplets: (object; relation; object) with this relations \\n (Movie title; Directed_by; Director)\t\\n (Movie title; Written_by; Writer1)\t\\n (Movie title; Written_by; Writer2) \\n . \\n . \\n . \\n (Movie title; Written_by; WriterN) \\n (Movie title; Produced_by; Producer person)\t\\n (Movie title; Edited_by; Editor)\t\\n (Movie title; Production_company; company)\t\\n (Movie title; Distributed_by\t; distributer)\t\\n (Movie title; Release_date; date)\t\\n (Movie title; Cast; Actor) \\n\"\n","  query_result = \"\"\n","  assistant = \"Here is the text: \\n\" + page_text + \"\\n and Here is the triplets we want to extract for this texte: \" + relations\n","  question = \"Extract triplets from the text using the provided ontology. Replace the movie title, by the movie name as an entity. \\n As response, only give the list of the triplets\"\n","\n","\n","\n","  messages = [ {\"role\": \"system\", \"content\": \"You are an expert in Extracting Knowledge Graph relation\"},\n","              {\"role\": \"assistant\", \"content\": assistant},\n","              {\"role\": \"user\", \"content\": question},\n","              ]\n","\n","  response = client.chat.completions.create(\n","      model=\"gpt-3.5-turbo\",\n","      messages=messages,\n","      temperature=0\n","  )\n","  return response.choices[0].message.content\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"QtWyT8kN46zC"},"outputs":[],"source":["films_entities_relations = []\n","for content in films_content:\n","  films_entities_relations.append(extract_entity_relation(content))"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"sT6PRK1-qwH1"},"outputs":[],"source":["import re\n","from neo4j import GraphDatabase\n","\n","def extract_between_parentheses(line):\n","    match = re.search(r'\\((.*?)\\)', line)\n","    if match:\n","        return match.group(1)\n","    else:\n","        return None\n","\n","def createneo4j_graph(string):\n","    # Initialiser une session Neo4j\n","    uri = \"\"\n","    user = \"\"\n","    password = \"\"\n","    driver = GraphDatabase.driver(uri, auth=(user, password))\n","\n","    with driver.session() as session:\n","        # Pour chaque ligne dans la chaîne de caractères\n","        for line in string.split('\\n'):\n","            # Extraire ce qui se trouve entre parenthèses\n","            extracted = extract_between_parentheses(line)\n","            if extracted:\n","                movie, relation, value = extracted.split(';')\n","                type_relation = None\n","                movie = movie.strip()\n","                relation = relation.strip()\n","                value = value.strip()\n","                # Créer les nœuds et les relations dans la base de données Neo4j\n","                if relation in ['Directed_by', 'Written_by', 'Produced_by', 'Edited_by', 'Cast']:\n","                  type_relation = 'Person'\n","                  property_name = 'name'\n","                elif relation in ['Edited_by', 'Production_company', 'Distributed_by']:\n","                  type_relation = 'Company'\n","                  property_name = 'name'\n","                elif relation in ['Release_date']:\n","                  type_relation = 'Date'\n","                  property_name = 'date'\n","                session.run(\n","                    \"MERGE (m:Movie {title: $movie})\"\n","                    \"MERGE (p:\" + type_relation+ \" {\"+property_name+\": $value})\"\n","                    \"MERGE (m)<-[:%s]-(p)\" % relation,\n","                    movie=movie,\n","                    value=value)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"6sZrqRfQrcaI"},"outputs":[],"source":["for entities_relations in films_entities_relations:\n","  createneo4j_graph(entities_relations)"]}],"metadata":{"colab":{"provenance":[]},"kernelspec":{"display_name":"Python 3","name":"python3"},"language_info":{"name":"python"}},"nbformat":4,"nbformat_minor":0}
